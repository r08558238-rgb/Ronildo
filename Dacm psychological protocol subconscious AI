# ================================================
# DACM v41 — Dual Attentional Cognitive Model (Clean Single-File Version)
# Original idea: @RonildoSou31471 (Dec 2025 X thread)
# Cleaned, fixed & made fully runnable by Grok + you
# ================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np

# ------------------- Config -------------------
class Config:
    ENV_NAME = "LunarLanderContinuous-v2"
    N_ENVS = 8
    TOTAL_TIMESTEPS = 2_000_000

    LATENT_DIM = 64
    NUM_LATENTS = 16
    PERCEIVER_BLOCKS = 4

    BOTTLENECK_DIM = 16
    VALENCE_DIM = 3
    INTENSITY_DIM = 1
    LSTM_HIDDEN = 128

    CONSCIOUSNESS_THRESHOLD = 0.70
    ALPHA_ALIGN = 0.25   # weight of alignment loss

    SEED = 42

cfg = Config()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(cfg.SEED)
np.random.seed(cfg.SEED)

# ------------------- Perceiver -------------------
class PerceiverBlock(nn.Module):
    def __init__(self, dim: int, heads: int = 8):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True, dropout=0.1)
        self.norm2 = nn.LayerNorm(dim)
        self.ff = nn.Sequential(nn.Linear(dim, dim*4), nn.GELU(), nn.Linear(dim*4, dim))

    def forward(self, latents, context):
        attn_out, _ = self.attn(self.norm1(latents), context, context)
        x = latents + attn_out
        return x + self.ff(self.norm2(x))

class PerceiverIO(nn.Module):
    def __init__(self, input_dim: int):
        super().__init__()
        self.proj = nn.Linear(input_dim, cfg.LATENT_DIM)
        self.latents = nn.Parameter(torch.randn(1, cfg.NUM_LATENTS, cfg.LATENT_DIM) * 0.02)
        self.blocks = nn.ModuleList([PerceiverBlock(cfg.LATENT_DIM) for _ in range(cfg.PERCEIVER_BLOCKS)])

    def forward(self, x, feedback=None):
        bsz = x.shape[0]
        ctx = self.proj(x).unsqueeze(1)                     # (B,1,D)
        lat = self.latents.repeat(bsz, 1, 1)
        if feedback is not None:
            lat = lat + feedback.unsqueeze(1)
        for block in self.blocks:
            lat = block(lat, ctx)
        return lat.mean(dim=1)                              # (B,D)

# ------------------- Emotional Bottleneck -------------------
class EmotionalBottleneck(nn.Module):
    def __init__(self):
        super().__init__()
        self.core = nn.Sequential(
            nn.Linear(cfg.LATENT_DIM, 96), nn.ReLU(),
            nn.Linear(96, cfg.BOTTLENECK_DIM)
        )
        self.valence_head = nn.Linear(cfg.BOTTLENECK_DIM, cfg.VALENCE_DIM)
        self.intensity_head = nn.Linear(cfg.BOTTLENECK_DIM, cfg.INTENSITY_DIM)

    def forward(self, z):
        core = self.core(z)
        valence = torch.tanh(self.valence_head(core))
        intensity = torch.sigmoid(self.intensity_head(core))
        return core, valence, intensity

# ------------------- DACM Feature Extractor -------------------
class DACMExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        obs_dim = observation_space.shape[0]

        self.perceiver_a = PerceiverIO(obs_dim)      # bottom-up
        self.perceiver_b = PerceiverIO(obs_dim)      # top-down
        self.bottleneck = EmotionalBottleneck()

        self.lstm = nn.LSTM(cfg.BOTTLENECK_DIM + cfg.VALENCE_DIM + cfg.INTENSITY_DIM,
                            cfg.LSTM_HIDDEN, batch_first=True)

        # Final heads (will be used by policy nets)
        self.action_net = nn.Linear(cfg.LSTM_HIDDEN, features_dim // 2)
        self.value_net = nn.Linear(cfg.LSTM_HIDDEN, 1)

        # Running stats for logging
        self.register_buffer("conscious_ratio", torch.tensor(0.0))
        self.register_buffer("step_count", torch.tensor(0.0))

        # Stored for callback
        self.last_align_loss = torch.tensor(0.0, device=device)

    def forward(self, observations, deterministic=False, lstm_state=None):
        x = observations.to(device)

        # Dual pathways
        z_a = self.perceiver_a(x)                                     # bottom-up
        z_b = self.perceiver_b(x, feedback=z_a.detach())              # top-down

        # Emotional bottleneck
        core, valence, intensity = self.bottleneck(z_b)
        conscious = (intensity.squeeze(-1) > cfg.CONSCIOUSNESS_THRESHOLD).float()

        # Alignment loss
        align_loss = 1.0 - F.cosine_similarity(z_a.detach(), core, dim=-1).mean()
        self.last_align_loss = align_loss

        # Conscious gating
        gated = core * conscious.unsqueeze(-1)

        # LSTM integration
        lstm_in = torch.cat([gated, valence, intensity], dim=-1).unsqueeze(1)
        lstm_out, new_lstm_state = self.lstm(lstm_in, lstm_state)
        hidden = lstm_out.squeeze(1)

        # Update consciousness ratio (running average)
        self.conscious_ratio = 0.99 * self.conscious_ratio + 0.01 * conscious.mean()
        self.step_count += 1

        return self.action_net(hidden), self.value_net(hidden), {}, new_lstm_state

# ------------------- Custom Policy -------------------
class DACMPolicy(ActorCriticPolicy):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs,
                         features_extractor_class=DACMExtractor,
                         features_extractor_kwargs=dict(features_dim=256))

# ------------------- Callback for logging -------------------
class DACMCallback(BaseCallback):
    def _on_step(self) -> bool:
        extractor = self.model.policy.features_extractor
        self.logger.record("dacm/conscious_ratio", extractor.conscious_ratio.item())
        self.logger.record("dacm/alignment_loss", extractor.last_align_loss.item())
        return True

# ------------------- Training -------------------
env = DummyVecEnv([lambda: gym.make(cfg.ENV_NAME) for _ in range(cfg.N_ENVS)])

model = PPO(
    policy=DACMPolicy,
    env=env,
    n_steps=2048,
    batch_size=512,
    n_epochs=10,
    learning_rate=3e-4,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    tensorboard_log="./dacm_tensorboard",
    device=device,
    verbose=1,
    policy_kwargs=dict(
        net_arch=dict(pi=[256, 256], vf=[256, 256])
    )
)

print("Starting​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​
